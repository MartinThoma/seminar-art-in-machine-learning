%!TEX root = art-in-machine-learning.tex

\section{Basics of Machine Learning}
\label{sec:ml-basics}
The traditional approach of solving problems with software is to program
machines to do so. The task is divided in as simple sub-tasks as possible,
the subtasks are analyzed and the machine is instructed to process the input
with human-designed algorithms to produce the desired output. However, for
some tasks like object recognition this approach is not feasable. There are
way to many different objects, different lightnig situations, variations in
rotation and the arrangement of a scene for a human to think of all of them and
model them. But with the internet, cheap computers, cameras, crowd-sourcing
platforms like Wikipedia and lots of Websites, services like Amazon Mechanical
Turk and several other changes in the past decades a lot of data has become
available. The idea of machine learning is to make use of this data.

A formal definition of the field of Machine Learning is given by
Tom~Mitchel~\cite{Mitchell97}:
\begin{displayquote}
A computer program is said to learn from experience~$E$ with respect to some
class of tasks~$T$ and performance measure~$P$, if its performance at tasks
in~$T$, as measured by~$P$, improves with experience $E$.
\end{displayquote}

This means that machine learning programs adjust internal parameters to fit the
data they are given. Those computer programs are still developed by software
developers, but the developer writes them in a way which makes it possible to
adjust them without having to re-program everything. Machine learning programs
should generally improve when they are fed with more data.

The field of machine learning is related to statistics. Some algorithms
directly try to find models which are based on distribution assumptions of the
developer, others are more general.

A common misunderstanding of people who are not related in this field is that
the developers don't understand what their machine learning program is doing.
It is understood very well in the sense that the developer, given only a pen,
lots of paper and a calculator could calculate the same result as the machine
does when he gets the same data. And lots of time, of course. It is not
understood in the sense that it is hard to make predictions how the algorithm
behaves without actually trying it. However, this is similar to expecting from
an electrical engineer to explain how a computer works. The electrical engineer
could probably get the knowledge he needs to do so, but the amount of time
required to understand such a complex system from basic building blocks is
a time-intensive and difficult task.

An important group of machine learning algorithms was inspired by biological
neurons and are thus called \textit{artifical neural networks}. Those networks
are based on mathematical functions called \textit{artifical neurons} which
take $n \in \mathbb{N}$ numbers $x_1, \dots, x_n \in \mathbb{R}$ as input,
multiply them with weights $w_1, \dots, w_n \in \mathbb{R}$, add them and apply
a so called \textit{activation function} $\varphi$. One example of such a
function is the sigmoid function $\varphi(x) = \frac{1}{1+e^{-x}}$. Those
functions act as building blocks for more complex systems as they can be
chained and grouped in layers. The interesting question is how the parameters
$w_i$ are learned. This is usually done by an optimization technique called
\textit{gradient descent}. The gradient descent algorithm takes a function
which has to be derivable, starts at any point of the surface of this error
function and makes a step in the direction which goes downwards. Hence it tries
to find a minimum of this high-dimensional function.

There is, of course, a lot more to say about machine learning. The interested
reader might want to read the introduction given by Mitchell~\cite{Mitchell97}.

%!TEX root = art-in-machine-learning.tex

\section{Audio Data}
\label{sec:music}

Common machine learning tasks which involve audio data are speech recognition,
speaker identification, identification of songs. This leads to some
less-common, but interesting topics: The composition of music, the synthesizing
of audio as art. While the composition might be considered in
\cref{sec:text-generation}, we will now investigate the work which was done in
audio synthesization.


\subsection{Emily Howell}
David Cope created a project called \enquote{Experiments in Musical
Intelligence} (short: EMI or Emmy) in 1984~\cite{Cope1987}. He introduces the idea of
seeing music as a language which can be analyzed with natural language
processing (NLP) methods. Cope mentions that EMI was more useful to him, when
he used the system to \enquote{create small phrase-size textures as next
possibilities using its syntactic dictionary and rule base}~\cite{Cope1987}.

In 2003, Cope started a new project which was based on EMI: Emily
Howell~\cite{cope2013well}. This program is able to \enquote{creat[e] both
highly authentic replications and novel music compositions}. The reader might
want to listen to~\cite{Cope2012} to get an impression of the beauty of the
created music.

According to Cope, an essential part of music is \enquote{a set of instructions
for creating different, but highly related self-replications}. Emmy was
programmed to find this set of instructions. It tries to find the
\enquote{signature} of a composer, which Cope describes as \enquote{contiguous
patterns that recur in two or more works of the composer}.

The new feature of \textit{Emily Howell} compared to \textit{Emmy} is that
Emily Howell does not necessarily remain in a single, already known style.

Emily Howell makes use of association network. Cope emphasizes that this is not
a form of a neural network. However, it is not clear from~\cite{cope2013well}
how exactly an association network is trained. Cope mentions that Emily
Howell is explained in detail in~\cite{cope2005computer}.


\subsection{GRUV}

Recurrent neural networks --- \gls{LSTM} networks, to be exact --- are used
in~\cite{nayebigruv} together with \gls{GRU} to build a network which can be
trained to generate music. Instead of taking notes directly or MIDI files,
Nayebi and Vitelli took raw audio waveforms as input. Those audio waveforms are
feature vectors given for time steps $0, 1, \dots, t-1, t$. The network is
given those feature vectors $X_1, \dots, X_t$ and has to predict the following
feature vector $X_{t+1}$. This means it continues the music. As the input is
continuous, the problem was modeled as a regression task. \Gls{DFT} was used on
chunks of length $N$ of the music to obtain features in the frequency domain.

An implementation can be found at~\cite{gruvGitHub} and a demonstration can
be found at~\cite{Vitelli2015}.


\subsection{Audio Synthesization}
Audio synthesization is generating new audio files. This can either be music or
speech. With the techniques described before, neural networks can be trained to
generate music note by note. However, it is desirable to allow multiple notes
being played at the same time.

This idea and some others were applied by Daniel Johnson. He wrote a very good
introduction into neural networks for music composition which explains those
ideas~\cite{Johnson2015}. Example compositions are available there, too. He
also made the code for his Biaxial Recurrent Neural Network available
under~\cite{Johnson2015a}.

% see also: \cite{sarroff2014musical}
% http://gitxiv.com/posts/dQxgWraeWgvpKJXLG/musical-audio-synthesis-using-autoencoding-neural-nets

% Deep Belief Network
% \href{http://gitxiv.com/posts/kZz9PCRcktdYSrWnp/deephear-composing-and-harmonizing-music-with-neural}{http://gitxiv.com/posts/kZz9PCRcktdYSrWnp/deephear-composing-and-harmonizing-music-with-neural}


% \subsection{The Generative Electronica Research Project}

% \href{http://metacreation.net/gerp/}{http://metacreation.net/gerp/}
% TODO: \cite{Eigenfeldt2013}

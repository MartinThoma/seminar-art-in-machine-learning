%!TEX root = art-in-machine-learning.tex

\section{Text Data}%
\label{sec:text-generation}%

Digital text is the first form of natural communication which involved
computers. It is used in the form of chats, websites, on collaborative projects
like Wikipedia, in scientific literature. Of course, it was used in pre-digital
times, too: In newspaper, in novels, in dramas, in religious texts like the
bible, in books for education, in notes from conversations.

This list could be continued, but there are some interesting sub-categories
which seem to be more creative than the others. And most of them are now
available in digital form. This digital form can be used to teach machines
to generate similar texts.

The most simple language model which is of use is an $n$-gram model. This model
makes use of sequences of the length $n$ to model language. It can be used to
get the probability of a third word, given the previous two words. This way, a
complete text can be generated word by word. Refinements and extensions to this
model are discussed in the field of \gls{NLP}.

However, there are much more sophisticated models. One of those are
\textit{character predictors} based on \glspl{RNN}. Those character predictors
take a sequence of characters as input and predict the next character. In that
sense they are similar to the $n$-gram model, but operate on a lower level.
Using such a predictor, one can generate texts character by character. If the
model is good, the text can have the correct punctuation. This would not be
possible with a word predictor.

Character predictors can be implemented with \glspl{RNN}. In contrast to
standard feed-forward neural networks like \glspl{MLP}, those networks are
trained to take their output at some point as well as the normal input. One of
the most common variant to implement \glspl{RNN} is by using so called
\gls{LSTM} cells~\cite{hochreiter1997long}.

Recurrent networks apply two main ideas in order to learn: The first is called
\textit{unrolling} and means that an recurrent network is imagined to be
an infinite network over time. At each time step the recurrent neurons get
duplicated. The second idea is \textit{weight sharing} which means that those
unrolled neurons share the same weight.


\subsection{Similar Texts Generation}
Karpathy trained multiple character \glspl{RNN} on different datasets and gave
an excellent introduction~\cite{Karpathy2015}. He trained it on Paul Graham's
essays, all the works of Shakespeare, the Hutter Prize~\cite{hutterPrize}
\SI{100}{\mega\byte}~dataset of raw Wikipedia articles, the raw \LaTeX{} source
file of a book about algebraic stacks and geometry and Linux C~code.

With that training data, the models can generate similar texts. New works which
look like Shakespeare plays, new Wikipedia articles, new Linux code and new
papers about algebraic geometry can thus automatically be generated. At a first
glance, they do look authentic. The syntax was mostly used correctly, the
formatting looks as expected, the sentences are grammatically correct. However,
when one looks at the broader context it is easy to recognize that the
algorithm has no insight in what it is doing. It does match patterns really
well, but it fails to follow a central theme. In the context of C~code this
means that new variables are introduced, but not used. At the same time,
variables which were not declared are used. In the context of Shakespear plays
this means that a lot of new characters are introduced, but they don't speak
with each other or about each other.

The code used to generate these examples is available and ready to use
through~\cite{charRNNgithub}. A couple of examples are
in~\cref{sec:appendix-auto-generated-text}.


\subsection{Chatbots}%
\label{subsec:chatbots}%

Chatbots are computer programs which participate in chat rooms as autonomous
agents. This means they have similar permissions and possibilities as usual human
users have, but users can trigger a set of commands to make the bot give them
valuable information or features.

A special category of chatbots are such bots which actively participate in the
conversation, which is usually not the case. One of the earliest programs in
this category is ELIZA, a bot created by Joseph Weizenbaum in the
1960s~\cite{Weizenbaum1976}. This program had a set of patterns implemented to
which it would reply in a seemingly smart way in a psychologists fashion. This
means quite often the program would simply repeat the last sentence and add
something meaningless like \enquote{How do you feel about it?}. According
to~\cite{Curtis2014}, Weizenbaum once found his secretary --- who was aware of
the fact that this is a computer program --- chatting with the machine. When he
looked over her shoulder, she asked him \enquote{would you mind leaving the
room}.

Today, much more sophisticated chatbots exist. They make use of the vast amount
of data which is available by millions of Wikipedia articles, chat protocols,
websites, help desk protocols, subtitles of movies as well as the astonishing
increase in computing power to train
\glspl{RNN} and language models similar to the ones described before.

Interesting results like the following were obtained
by~\cite{vinyals2015neural}:
\begin{displayquote}
\textbf{Human}: what is the purpose of life ?\\
\textbf{Machine}: to serve the greater good .\\
\textbf{Human}: what is the purpose of living ?\\
\textbf{Machine}: to live forever .
\end{displayquote}
